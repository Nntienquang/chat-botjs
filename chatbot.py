"""
Chatbot s·ª≠ d·ª•ng Groq API (Llama 3.1) ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ t√†i li·ªáu
"""
import os
import json
import re
from typing import List, Tuple, Optional
from docx import Document
import PyPDF2
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from groq import Groq

# Groq API (FREE - Llama 3.1)
# L·∫•y t·ª´ bi·∫øn m√¥i tr∆∞·ªùng (b·∫Øt bu·ªôc)
GROQ_API_KEY = os.environ.get("GROQ_API_KEY", "")

class DocumentChatbot:
    def __init__(self, doc_folder: str = "doc"):
        """
        Kh·ªüi t·∫°o chatbot v·ªõi Groq API (Llama 3.1)
        """
        self.doc_folder = doc_folder
        self.documents = []
        self.document_metadata = []
        self.chunks = []
        self.embeddings = None
        self.model = None
        
        # Kh·ªüi t·∫°o Groq client
        try:
            self.groq_client = Groq(api_key=GROQ_API_KEY)
            print(f"‚úì ƒê√£ k·∫øt n·ªëi Groq API th√†nh c√¥ng! (Key: {GROQ_API_KEY[:10]}...)")
        except Exception as e:
            print(f"‚ùå L·ªói k·∫øt n·ªëi Groq API: {e}")
            self.groq_client = None
        
        print("ƒêang t·∫£i m√¥ h√¨nh embedding...")
        try:
            self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        except:
            self.model = SentenceTransformer('all-MiniLM-L6-v2')
        print("‚úì ƒê√£ t·∫£i m√¥ h√¨nh embedding th√†nh c√¥ng!")
        
        # T·∫£i Q&A dataset n·∫øu c√≥
        self.load_qa_dataset()
        
        print("Chatbot ƒë√£ s·∫µn s√†ng v·ªõi Groq API (Llama 3.1)!")
    
    def load_qa_dataset(self):
        """T·∫£i Q&A dataset"""
        qa_file = "qa_dataset.json"
        if os.path.exists(qa_file):
            try:
                with open(qa_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.qa_dataset = data.get('questions', [])
                    print(f"‚úì ƒê√£ t·∫£i {len(self.qa_dataset)} c√¢u h·ªèi-ƒë√°p t·ª´ dataset!")
            except Exception as e:
                print(f"Kh√¥ng th·ªÉ t·∫£i Q&A dataset: {e}")
                self.qa_dataset = []
        else:
            self.qa_dataset = []
    
    def read_docx(self, file_path: str) -> str:
        """ƒê·ªçc file .docx"""
        try:
            doc = Document(file_path)
            text = []
            
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    text.append(paragraph.text.strip())
            
            # ƒê·ªçc tables
            for table in doc.tables:
                for row in table.rows:
                    row_text = []
                    for cell in row.cells:
                        if cell.text.strip():
                            row_text.append(cell.text.strip())
                    if row_text:
                        text.append(" | ".join(row_text))
            
            return "\n\n".join(text)
        except Exception as e:
            print(f"L·ªói ƒë·ªçc file {file_path}: {e}")
            return ""
    
    def read_pdf(self, file_path: str) -> str:
        """ƒê·ªçc file .pdf"""
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text = []
                for page in pdf_reader.pages:
                    text.append(page.extract_text())
                return "\n\n".join(text)
        except Exception as e:
            print(f"L·ªói ƒë·ªçc file {file_path}: {e}")
            return ""
    
    def read_txt(self, file_path: str) -> str:
        """ƒê·ªçc file .txt"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"L·ªói ƒë·ªçc file {file_path}: {e}")
            return ""
    
    def load_documents(self):
        """ƒê·ªçc t·∫•t c·∫£ t√†i li·ªáu t·ª´ folder doc"""
        if not os.path.exists(self.doc_folder):
            print(f"Folder {self.doc_folder} kh√¥ng t·ªìn t·∫°i!")
            return
        
        print(f"ƒêang ƒë·ªçc t√†i li·ªáu t·ª´ folder {self.doc_folder}...")
        
        import glob
        docx_files = glob.glob(os.path.join(self.doc_folder, "*.docx"))
        pdf_files = glob.glob(os.path.join(self.doc_folder, "*.pdf"))
        txt_files = glob.glob(os.path.join(self.doc_folder, "*.txt"))
        
        all_files = docx_files + pdf_files + txt_files
        
        for file_path in all_files:
            file_name = os.path.basename(file_path)
            if file_name.startswith("~$"):  # B·ªè qua file temp
                continue
            
            print(f"ƒêang ƒë·ªçc: {file_name}")
            
            if file_path.endswith('.docx'):
                content = self.read_docx(file_path)
            elif file_path.endswith('.pdf'):
                content = self.read_pdf(file_path)
            elif file_path.endswith('.txt'):
                content = self.read_txt(file_path)
            else:
                continue
            
            if content.strip():
                # Chia th√†nh chunks
                chunks = self.split_text(content)
                for chunk in chunks:
                    self.chunks.append(chunk)
                    self.document_metadata.append({
                        'file': file_name,
                        'path': file_path
                    })
        
        print(f"ƒê√£ ƒë·ªçc {len(self.chunks)} chunks t·ª´ {len(all_files)} files")
        
        # T·∫°o embeddings
        if self.chunks:
            print("ƒêang t·∫°o embeddings...")
            self.embeddings = self.model.encode(self.chunks, show_progress_bar=True)
            print("‚úì ƒê√£ t·∫°o embeddings th√†nh c√¥ng!")
    
    def split_text(self, text: str, chunk_size: int = 800) -> List[str]:
        """Chia text th√†nh chunks"""
        # Chia theo ƒëo·∫°n vƒÉn tr∆∞·ªõc
        paragraphs = re.split(r'\n\s*\n', text)
        chunks = []
        current_chunk = ""
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
            
            if len(current_chunk) + len(para) < chunk_size:
                current_chunk += "\n\n" + para if current_chunk else para
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = para
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
    
    def search(self, query: str, top_k: int = 5) -> List[Tuple[str, float]]:
        """T√¨m ki·∫øm semantic trong t√†i li·ªáu"""
        if not self.chunks or self.embeddings is None:
            return []
        
        query_embedding = self.model.encode([query])
        similarities = cosine_similarity(query_embedding, self.embeddings)[0]
        
        # L·∫•y top_k k·∫øt qu·∫£
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            if similarities[idx] > 0.15:  # Ng∆∞·ª°ng t·ªëi thi·ªÉu (gi·∫£m ƒë·ªÉ l·∫•y nhi·ªÅu k·∫øt qu·∫£ h∆°n)
                results.append((self.chunks[idx], float(similarities[idx])))
        
        return results
    
    def search_context(self, query: str, max_chunks: int = 5) -> str:
        """T√¨m v√† k·∫øt h·ª£p c√°c ƒëo·∫°n li√™n quan nh·∫•t ƒë·ªÉ l√†m context"""
        # TƒÉng s·ªë chunks ƒë·ªÉ c√≥ ƒë·ªß context
        results = self.search(query, top_k=max_chunks)
        
        if not results:
            return ""
        
        # K·∫øt h·ª£p c√°c chunks theo ƒë·ªô li√™n quan, c√≥ ƒëi·ªÉm s·ªë ƒë·ªÉ ∆∞u ti√™n
        context_parts = []
        for chunk, score in results:
            # Ch·ªâ l·∫•y chunks c√≥ ƒë·ªô li√™n quan ƒë·ªß cao
            if score > 0.2:  # Ng∆∞·ª°ng t·ªëi thi·ªÉu
                context_parts.append(f"[ƒê·ªô li√™n quan: {score:.2f}]\n{chunk}")
        
        if not context_parts:
            # N·∫øu kh√¥ng c√≥ chunk n√†o ƒë·ªß ƒëi·ªÉm, v·∫´n l·∫•y chunk t·ªët nh·∫•t
            context_parts.append(results[0][0])
        
        return "\n\n---\n\n".join(context_parts)
    
    def find_qa_match(self, question: str) -> Optional[str]:
        """T√¨m c√¢u tr·∫£ l·ªùi t·ª´ Q&A dataset"""
        if not self.qa_dataset:
            return None
        
        question_lower = question.lower().strip()
        
        # T√¨m ki·∫øm ch√≠nh x√°c
        for qa in self.qa_dataset:
            if qa['question'].lower().strip() == question_lower:
                return qa['answer']
        
        # T√¨m ki·∫øm theo t·ª´ kh√≥a
        question_words = set(re.findall(r'\b\w{2,}\b', question_lower))
        best_match = None
        best_score = 0
        
        for qa in self.qa_dataset:
            qa_keywords = set(qa.get('keywords', []))
            keyword_match = len(question_words & qa_keywords)
            
            if keyword_match > best_score and keyword_match > 0:
                best_score = keyword_match
                best_match = qa['answer']
        
        return best_match if best_score > 1 else None
    
    def call_groq_api(self, question: str, context: str = "") -> Optional[str]:
        """G·ªçi Groq API (Llama 3.1) ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi - CH·ªà d·ª±a tr√™n t√†i li·ªáu"""
        if not self.groq_client:
            print("‚ùå Groq client ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o!")
            return None
        
        if not context or len(context.strip()) < 10:
            print("‚ö†Ô∏è Context qu√° ng·∫Øn, kh√¥ng ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi")
            return None
        
        try:
            # Gi·ªõi h·∫°n context ƒë·ªÉ tr√°nh qu√° d√†i, nh∆∞ng ƒë·∫£m b·∫£o ƒë·ªß th√¥ng tin
            context_limited = context[:4000] if len(context) > 4000 else context
            
            print(f"üîÑ ƒêang g·ªçi Groq API v·ªõi c√¢u h·ªèi: {question[:50]}...")
            print(f"üìÑ Context length: {len(context_limited)} k√Ω t·ª±")
            
            # Prompt r·∫•t nghi√™m ng·∫∑t ƒë·ªÉ b·∫Øt bu·ªôc ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n t√†i li·ªáu
            system_prompt = """B·∫°n l√† chatbot b√†i gi·∫£ng. QUY T·∫ÆC NGHI√äM NG·∫∂T:
1. CH·ªà tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin C√ì S·∫¥N trong t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p
2. KH√îNG ƒë∆∞·ª£c th√™m b·∫•t k·ª≥ th√¥ng tin n√†o kh√¥ng c√≥ trong t√†i li·ªáu
3. N·∫øu t√†i li·ªáu kh√¥ng c√≥ th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi, h√£y n√≥i r√µ "T√†i li·ªáu kh√¥ng c√≥ th√¥ng tin v·ªÅ..."
4. Tr√≠ch d·∫´n ch√≠nh x√°c t·ª´ t√†i li·ªáu khi c√≥ th·ªÉ
5. Tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß, r√µ r√†ng, m·∫°ch l·∫°c nh∆∞ng TUY·ªÜT ƒê·ªêI kh√¥ng th√™m th√¥ng tin ngo√†i"""
            
            user_prompt = f"""ƒê√ÇY L√Ä TO√ÄN B·ªò TH√îNG TIN T√ÄI LI·ªÜU (CH·ªà D·ª∞A V√ÄO ƒê√ÇY ƒê·ªÇ TR·∫¢ L·ªúI):

{context_limited}

---
C√ÇU H·ªéI: {question}

L∆ØU √ù QUAN TR·ªåNG:
- CH·ªà tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin TR√äN ƒê√ÇY
- N·∫øu th√¥ng tin kh√¥ng c√≥ trong t√†i li·ªáu tr√™n, h√£y n√≥i r√µ "T√†i li·ªáu kh√¥ng c√≥ th√¥ng tin v·ªÅ..."
- KH√îNG ƒë∆∞·ª£c suy ƒëo√°n, t∆∞·ªüng t∆∞·ª£ng, ho·∫∑c th√™m th√¥ng tin ngo√†i
- Tr√≠ch d·∫´n ch√≠nh x√°c t·ª´ t√†i li·ªáu khi c√≥ th·ªÉ

TR·∫¢ L·ªúI (ch·ªâ d·ª±a tr√™n t√†i li·ªáu):"""
            
            # G·ªçi Groq API v·ªõi Llama 3.1 - gi·∫£m temperature ƒë·ªÉ ch√≠nh x√°c h∆°n
            completion = self.groq_client.chat.completions.create(
                model="llama-3.1-8b-instant",
                messages=[
                    {
                        "role": "system",
                        "content": system_prompt
                    },
                    {
                        "role": "user",
                        "content": user_prompt
                    }
                ],
                temperature=0.3,  # Gi·∫£m t·ª´ 0.7 xu·ªëng 0.3 ƒë·ªÉ ch√≠nh x√°c h∆°n, √≠t "t∆∞·ªüng t∆∞·ª£ng" h∆°n
                max_tokens=600
            )
            
            answer = completion.choices[0].message.content.strip()
            print(f"‚úÖ Groq API tr·∫£ l·ªùi th√†nh c√¥ng! (ƒê·ªô d√†i: {len(answer)} k√Ω t·ª±)")
            
            # Ki·ªÉm tra xem c√¢u tr·∫£ l·ªùi c√≥ qu√° ng·∫Øn kh√¥ng
            if answer and len(answer) > 15:
                # L√†m s·∫°ch c√¢u tr·∫£ l·ªùi
                answer = answer.strip()
                # Lo·∫°i b·ªè c√°c ph·∫ßn c√≥ th·ªÉ l√† prompt c√≤n s√≥t l·∫°i
                if "TR·∫¢ L·ªúI:" in answer:
                    answer = answer.split("TR·∫¢ L·ªúI:")[-1].strip()
                return answer
            
            print("‚ö†Ô∏è C√¢u tr·∫£ l·ªùi t·ª´ Groq API qu√° ng·∫Øn")
            return None
                
        except Exception as e:
            print(f"‚ùå L·ªói khi g·ªçi Groq API: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def answer(self, question: str) -> str:
        """Tr·∫£ l·ªùi c√¢u h·ªèi s·ª≠ d·ª•ng Groq API"""
        question = question.strip()
        
        if len(question) < 3:
            return "Xin l·ªói, c√¢u h·ªèi c·ªßa b·∫°n qu√° ng·∫Øn. Vui l√≤ng ƒë·∫∑t c√¢u h·ªèi c·ª• th·ªÉ h∆°n."
        
        # T√¨m trong Q&A dataset tr∆∞·ªõc (nh∆∞ng v·∫´n log ƒë·ªÉ debug)
        qa_answer = self.find_qa_match(question)
        if qa_answer:
            print(f"üìö T√¨m th·∫•y trong Q&A dataset, b·ªè qua API call")
            return qa_answer
        
        # T√¨m ki·∫øm context li√™n quan nh·∫•t - tƒÉng s·ªë chunks ƒë·ªÉ c√≥ ƒë·ªß th√¥ng tin
        context = self.search_context(question, max_chunks=5)
        
        if not context:
            return f"Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin v·ªÅ '{question}' trong t√†i li·ªáu. H√£y th·ª≠ ƒë·∫∑t c√¢u h·ªèi kh√°c."
        
        print(f"üìö ƒê√£ t√¨m th·∫•y context t·ª´ t√†i li·ªáu ({len(context)} k√Ω t·ª±)")
        
        # G·ªçi Groq API (Llama 3.1) ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi
        answer = self.call_groq_api(question, context)
        
        if answer and len(answer) > 20:
            # L√†m s·∫°ch c√¢u tr·∫£ l·ªùi
            answer = answer.strip()
            # Lo·∫°i b·ªè c√°c k√Ω t·ª± l·∫°
            answer = re.sub(r'\s+', ' ', answer)
            return answer
        
        # Fallback: T·ªïng h·ª£p t·ª´ c√°c chunks t·ªët nh·∫•t
        results = self.search(question, top_k=3)
        if results:
            if len(results) > 1:
                combined = "\n\n".join([chunk for chunk, score in results[:2]])
                return f"D·ª±a tr√™n t√†i li·ªáu:\n\n{combined[:500]}..."
            else:
                return results[0][0][:500] + "..." if len(results[0][0]) > 500 else results[0][0]
        
        return f"Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin v·ªÅ '{question}' trong t√†i li·ªáu."

