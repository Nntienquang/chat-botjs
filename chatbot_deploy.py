"""
Chatbot s·ª≠ d·ª•ng Groq API (Llama 3.1)
B·∫£n T·ªêI ∆ØU deploy ‚Äì KH√îNG load docx, KH√îNG t·∫°o embedding
Ch·ªâ ƒë·ªçc 2 file: embeddings.npy + chunks.json
"""
import os
import json
import numpy as np
from groq import Groq
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import re
from typing import List, Tuple, Optional
from dotenv import load_dotenv

# Load bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env
load_dotenv()

GROQ_API_KEY = os.environ.get("GROQ_API_KEY", "")

class DocumentChatbot:
    def __init__(self):
        """Kh·ªüi t·∫°o chatbot - phi√™n b·∫£n deploy (nh·∫π)"""
        # Kh·ªüi t·∫°o Groq client
        if not GROQ_API_KEY:
            print("‚ùå GROQ_API_KEY ch∆∞a ƒë∆∞·ª£c set!")
            print("   Ki·ªÉm tra: Render ‚Üí Settings ‚Üí Environment Variables")
            self.groq_client = None
        else:
            try:
                self.groq_client = Groq(api_key=GROQ_API_KEY)
                print("‚úÖ ƒê√£ k·∫øt n·ªëi Groq API")
            except Exception as e:
                print(f"‚ùå L·ªói k·∫øt n·ªëi Groq API: {e}")
                self.groq_client = None
        
        # Load embeddings v√† chunks (ƒë√£ t·∫°o s·∫µn t·ª´ local)
        print("üîÑ ƒêang load embeddings (deploy version)...")
        try:
            self.chunks = json.load(open("chunks.json", encoding="utf-8"))
            # Load embeddings v·ªõi mmap_mode ƒë·ªÉ ti·∫øt ki·ªám memory
            self.embeddings = np.load("embeddings.npy", mmap_mode='r')
            print(f"‚úÖ Loaded {len(self.chunks)} chunks & embeddings shape: {self.embeddings.shape} (memory-mapped)")
        except FileNotFoundError as e:
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y embeddings.npy ho·∫∑c chunks.json!")
            print(f"   L·ªói: {e}")
            print("   Vui l√≤ng ch·∫°y generate_embeddings.py tr√™n local tr∆∞·ªõc!")
            self.chunks = []
            self.embeddings = None
        except Exception as e:
            print(f"‚ùå L·ªói khi load embeddings: {e}")
            self.chunks = []
            self.embeddings = None
        
        # KH√îNG load model ngay - lazy load khi c·∫ßn
        self.model = None
        print("‚úÖ Embeddings ƒë√£ s·∫µn s√†ng (model s·∫Ω load khi c·∫ßn)")
        
        # Load Q&A dataset n·∫øu c√≥
        self.load_qa_dataset()
    
    def load_qa_dataset(self):
        """T·∫£i Q&A dataset"""
        try:
            if os.path.exists("qa_dataset.json"):
                with open("qa_dataset.json", 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.qa_dataset = data.get('questions', [])
                    print(f"‚úÖ ƒê√£ t·∫£i {len(self.qa_dataset)} Q&A t·ª´ dataset")
            else:
                self.qa_dataset = []
        except Exception as e:
            print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ load Q&A dataset: {e}")
            self.qa_dataset = []
    
    def find_qa_match(self, question: str) -> Optional[str]:
        """T√¨m trong Q&A dataset"""
        if not self.qa_dataset:
            return None
        
        question_lower = question.lower().strip()
        
        # T√¨m ch√≠nh x√°c
        for qa in self.qa_dataset:
            if qa['question'].lower().strip() == question_lower:
                return qa['answer']
        
        # T√¨m theo keyword
        question_words = set(re.findall(r'\b\w{2,}\b', question_lower))
        best_match = None
        best_score = 0
        
        for qa in self.qa_dataset:
            qa_keywords = set(qa.get('keywords', []))
            keyword_match = len(question_words & qa_keywords)
            if keyword_match > best_score and keyword_match > 0:
                best_score = keyword_match
                best_match = qa['answer']
        
        return best_match if best_score > 1 else None
    
    def _get_model(self):
        """Lazy load model - ch·ªâ load khi c·∫ßn"""
        if self.model is None:
            print("üîÑ Lazy loading model embedding...")
            try:
                self.model = SentenceTransformer("all-MiniLM-L6-v2")
                print("‚úÖ Model ƒë√£ s·∫µn s√†ng")
            except Exception as e:
                print(f"‚ùå L·ªói khi load model: {e}")
                return None
        return self.model
    
    def search(self, query: str, top_k: int = 5) -> List[Tuple[str, float]]:
        """T√¨m ki·∫øm semantic"""
        if not self.chunks or self.embeddings is None:
            return []
        
        model = self._get_model()
        if model is None:
            return []
        
        try:
            # Encode query
            q_emb = model.encode([query], show_progress_bar=False)
            # T√≠nh similarity v·ªõi memory-mapped embeddings
            sim = cosine_similarity(q_emb, self.embeddings)[0]
            idx = np.argsort(sim)[::-1][:top_k]
            
            results = []
            for i in idx:
                if sim[i] > 0.15:  # Ng∆∞·ª°ng t·ªëi thi·ªÉu
                    results.append((self.chunks[i], float(sim[i])))
            return results
        except Exception as e:
            print(f"‚ùå L·ªói khi search: {e}")
            return []
    
    def build_context(self, question: str) -> str:
        """X√¢y d·ª±ng context t·ª´ k·∫øt qu·∫£ t√¨m ki·∫øm"""
        results = self.search(question, top_k=5)
        if not results:
            return ""
        
        parts = []
        for chunk, score in results:
            if score > 0.15:
                parts.append(chunk)
        
        return "\n\n---\n\n".join(parts) if parts else ""
    
    def call_groq(self, question: str, context: str) -> str:
        """G·ªçi Groq API"""
        if not self.groq_client:
            return "Xin l·ªói, chatbot ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh ƒë√∫ng. Vui l√≤ng ki·ªÉm tra GROQ_API_KEY."
        
        if not context:
            return "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin v·ªÅ c√¢u h·ªèi n√†y trong t√†i li·ªáu."
        
        try:
            system_msg = """B·∫°n l√† chatbot b√†i gi·∫£ng th√¥ng minh. Nhi·ªám v·ª• c·ªßa b·∫°n l√† ƒë·ªçc k·ªπ th√¥ng tin t√†i li·ªáu v√† tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch CH√çNH X√ÅC, ƒê·∫¶Y ƒê·ª¶, T·ª∞ NHI√äN v√† M·∫†CH L·∫†C.

QUY T·∫ÆC NGHI√äM NG·∫∂T:
1. CH·ªà tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin C√ì S·∫¥N trong t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p
2. KH√îNG ƒë∆∞·ª£c th√™m b·∫•t k·ª≥ th√¥ng tin n√†o kh√¥ng c√≥ trong t√†i li·ªáu
3. N·∫øu t√†i li·ªáu kh√¥ng c√≥ th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi, h√£y n√≥i r√µ "T√†i li·ªáu kh√¥ng c√≥ th√¥ng tin v·ªÅ..."
4. Tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß, r√µ r√†ng, m·∫°ch l·∫°c nh∆∞ng TUY·ªÜT ƒê·ªêI kh√¥ng th√™m th√¥ng tin ngo√†i"""
            
            user_msg = f"""ƒê√ÇY L√Ä TO√ÄN B·ªò TH√îNG TIN T√ÄI LI·ªÜU (CH·ªà D·ª∞A V√ÄO ƒê√ÇY ƒê·ªÇ TR·∫¢ L·ªúI):

{context[:3000]}

---
C√ÇU H·ªéI: {question}

L∆ØU √ù QUAN TR·ªåNG:
- CH·ªà tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin TR√äN ƒê√ÇY
- N·∫øu th√¥ng tin kh√¥ng c√≥ trong t√†i li·ªáu tr√™n, h√£y n√≥i r√µ "T√†i li·ªáu kh√¥ng c√≥ th√¥ng tin v·ªÅ..."
- KH√îNG ƒë∆∞·ª£c suy ƒëo√°n, t∆∞·ªüng t∆∞·ª£ng, ho·∫∑c th√™m th√¥ng tin ngo√†i

TR·∫¢ L·ªúI (ch·ªâ d·ª±a tr√™n t√†i li·ªáu):"""
            
            res = self.groq_client.chat.completions.create(
                model="llama-3.1-8b-instant",
                messages=[
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": user_msg}
                ],
                temperature=0.3,
                max_tokens=600
            )
            
            return res.choices[0].message.content.strip()
        except Exception as e:
            print(f"‚ùå L·ªói khi g·ªçi Groq API: {e}")
            return f"Xin l·ªói, ƒë√£ x·∫£y ra l·ªói khi x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}"
    
    def answer(self, question: str) -> str:
        """Tr·∫£ l·ªùi c√¢u h·ªèi"""
        question = question.strip()
        
        if len(question) < 3:
            return "Xin l·ªói, c√¢u h·ªèi c·ªßa b·∫°n qu√° ng·∫Øn. Vui l√≤ng ƒë·∫∑t c√¢u h·ªèi c·ª• th·ªÉ h∆°n."
        
        # T√¨m trong Q&A dataset tr∆∞·ªõc
        qa_answer = self.find_qa_match(question)
        if qa_answer:
            return qa_answer
        
        # T√¨m context v√† g·ªçi Groq API
        context = self.build_context(question)
        answer = self.call_groq(question, context)
        
        return answer

